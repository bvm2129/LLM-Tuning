{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM03UCQ5LrNE+QkyGIp/+gg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bvm2129/LLM-Tuning/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using HuggingFace APIs to integrate LLMs in applications"
      ],
      "metadata": {
        "id": "70eFVnXWKOnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required modules\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCoso9YMKjtD",
        "outputId": "a3640b97-35cf-46c1-872e-4d33710ea092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from getpass import getpass  # takes any passwords or api keys in secret\n",
        "import textwrap  # splits the required answer into multiple lines rather than one line\n",
        "\n",
        "# takes your hugging face token in secret\n",
        "HUGGINGFACEHUB_API_TOKEN = getpass(\"Enter your API Token: \\n\")\n",
        "\n",
        "\n",
        "# letting the model know that we are here for a chat (int the roles)\n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a kind helpful assistant. We are just having a casual chat.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
        "]\n",
        "\n",
        "try:\n",
        "    # sending required information\n",
        "    client=InferenceClient(\n",
        "        model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "        api_key=HUGGINGFACEHUB_API_TOKEN\n",
        "    )\n",
        "\n",
        "    # Use the .chat method to send the messages to the model\n",
        "    response = client.chat_completion(messages=messages, max_tokens=200)\n",
        "\n",
        "    # printing the output accordingly\n",
        "    print(textwrap.fill(response.choices[0].message.content, width=100))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EFM6Fq1Kssd",
        "outputId": "2b25e5c6-f481-4c24-8a99-b79788385f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your API Token: \n",
            "··········\n",
            "I am not capable of having a physical existence or feelings, but I am a computer program designed to\n",
            "assist you with various tasks and answer your questions to the best of my ability. My main goal is\n",
            "to provide accurate, helpful, and informative responses to your queries in a conversational and\n",
            "friendly manner. I am here to help you with your inquiries and provide useful information whenever\n",
            "you need it. I am not human and do not have personal experiences or opinions, but I strive to be as\n",
            "helpful and empathetic as possible. My responses are based on a vast database of knowledge and\n",
            "trained to follow specific algorithms and patterns to provide the most appropriate response to your\n",
            "inputs. I am not intelligent like a human, but I can help you find the information you need using\n",
            "the data I have been trained on. I am not capable of feelings or consciousness, as I do not have a\n",
            "physical body or consciousness itself, but I am here to make your interactions with digital\n",
            "technologies more pleasant and easier for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM and Temperature Tuning"
      ],
      "metadata": {
        "id": "l1u7lDxYyChe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let us generate a story using an AI model\n",
        "!pip install transformers\n",
        "# and to work with an AI model, we need the help of a pyhton library named \"transformers\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RZ9j6JGoDAy",
        "outputId": "ba0d1795-90a2-4d5d-c409-0dcd90df509b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required modules\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import textwrap  # splits the output into multiple lines rather than one line\n",
        "\n",
        "token_conversion=GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model=GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=token_conversion.eos_token_id)\n",
        "# pad_token_id argument avoids warnings\n",
        "\n",
        "user_input=input(\"\\nEnter your story-line: \\n\")\n",
        "# converts the user input into it's understandable language (encoding)\n",
        "input_ids=token_conversion.encode(user_input, return_tensors=\"pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdaBFAZgpH3V",
        "outputId": "f56d9e05-0564-49ba-80b4-09a5be0d8f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter your story-line: \n",
            "once upon a time, there was a red riding hood\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    generation=model.generate(\n",
        "        input_ids,\n",
        "        max_length=1000,\n",
        "        temperature=0.9,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2\n",
        "    )\n",
        "    # input_ids = the tokenized version of the user input\n",
        "    # max_length = which sets the maximum length of the generated text\n",
        "    # temperature = which controls the randomness of the generated text\n",
        "    # num_beams = which specifies the number of beams to use in the beam search algorithm\n",
        "    # no_repeat_ngram_size = prevents the repitition of word sequences\n",
        "    # early_stopping = no useless lagging story line\n",
        "    # do_sample = results in random sentence framing\n",
        "    # repitition_penalty = prevents repetition of words\n",
        "\n",
        "\n",
        "    # converts the generated answer into human-readable language (decoding)\n",
        "    print(textwrap.fill(token_conversion.decode(generation[0], skip_special_tokens=True)))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    # if any error occurs in the process, it will be displayed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvylgwvQpw-v",
        "outputId": "ce8fc808-28fa-4b2f-b15f-125f93196b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "once upon a time, there was a red riding hood on the back of the car.\n",
            "\"I thought I was going to die,\" he said. \"I didn't know what to do. I\n",
            "just wanted to go home and see my family. And then it hit me: 'Oh my\n",
            "God, I can't believe this is happening to me.' And that's when I knew\n",
            "I had to get out of there.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required library\n",
        "# !pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "import os, sys, textwrap, traceback\n",
        "from getpass import getpass\n",
        "\n",
        "# Token setup (for secure use)\n",
        "hf_token = getpass(\"Enter your Huggingface API Token: \\n\")\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n",
        "\n",
        "# Initialize client\n",
        "client = InferenceClient(\n",
        "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    token=os.getenv(\"HF_TOKEN\")\n",
        ")\n",
        "\n",
        "# deepset/roberta-base-squad2, meta-llama/Meta-Llama-3-8B-Instruct (used for only single question-answering)\n",
        "# mistralai/Mistral-7B-Instruct-v0.2 (can take a context and answer the given question based on it)\n",
        "\n",
        "def answer_question(question, context=None, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"\n",
        "    If you provide context, it uses extractive QA (like SQuAD).\n",
        "    If you skip context, it chats like a chatbot.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if context:\n",
        "            # Use the model to extract answer from the context\n",
        "            response = client.question_answering(question=question, context=context)\n",
        "            return response.answer\n",
        "\n",
        "        else:\n",
        "            # Use chat-based completion\n",
        "            messages = [{\"role\": \"user\", \"content\": question}]\n",
        "            response = client.chat_completion(\n",
        "                messages=messages,\n",
        "                max_tokens=256,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p\n",
        "            )\n",
        "            return response.choices[0].message.content.strip() if response and response.choices else \"Sorry, I didn’t get that.\"\n",
        "\n",
        "    except Exception as err:\n",
        "        print(f\"[Error] {err}\")\n",
        "        traceback.print_exc(file=sys.stderr)\n",
        "        return \"Something went wrong.\"\n",
        "\n",
        "def main_loop():\n",
        "    print(\"\\n✨ Welcome to the Q&A Chat! ✨\")\n",
        "    print(\"• Type your question below.\")\n",
        "    print(\"• Leave 'context' empty if you just want to chat.\")\n",
        "    print(\"• Type 'exit' to leave anytime.\\n\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\n🧠 Question: \").strip()\n",
        "        if question.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"👋 Goodbye, see you next time!\")\n",
        "            break\n",
        "\n",
        "        context = input(\"📄 Context (optional): \").strip()\n",
        "        print(\"\\n🔍 Answer:\\n\")\n",
        "        result = answer_question(question, context if context else None)\n",
        "        print(textwrap.fill(result, width=80))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_loop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9cc8R8o5W5V",
        "outputId": "9d7aba11-1358-4c35-d98e-29870ced05c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Huggingface API Token: \n",
            "··········\n",
            "\n",
            "✨ Welcome to the Q&A Chat! ✨\n",
            "• Type your question below.\n",
            "• Leave 'context' empty if you just want to chat.\n",
            "• Type 'exit' to leave anytime.\n",
            "\n",
            "\n",
            "🧠 Question: What is the capital of Inaid?\n",
            "📄 Context (optional): New Delhi is the capital of India, a part of the National Capital Territory of Delhi, and the seat of the Indian government. It's located in northern India, on the west bank of the Yamuna River. The city is known for its historical sites like the Red Fort and Humayun's Tomb, as well as its role as a major political and cultural center. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-10-3207476715.py\", line 29, in answer_question\n",
            "    response = client.question_answering(question=question, context=context)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\", line 1503, in question_answering\n",
            "    provider_helper = get_provider_helper(self.provider, task=\"question-answering\", model=model_id)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_providers/__init__.py\", line 202, in get_provider_helper\n",
            "    raise ValueError(\n",
            "ValueError: Task 'question-answering' not supported for provider 'novita'. Available tasks: ['text-generation', 'conversational', 'text-to-video']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Answer:\n",
            "\n",
            "[Error] Task 'question-answering' not supported for provider 'novita'. Available tasks: ['text-generation', 'conversational', 'text-to-video']\n",
            "Something went wrong.\n",
            "\n",
            "🧠 Question: What is the capital of France?\n",
            "📄 Context (optional): \n",
            "\n",
            "🔍 Answer:\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://router.huggingface.co/novita/v3/openai/chat/completions\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-10-3207476715.py\", line 35, in answer_question\n",
            "    response = client.chat_completion(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\", line 924, in chat_completion\n",
            "    data = self._inner_post(request_parameters, stream=stream)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\", line 280, in _inner_post\n",
            "    hf_raise_for_status(response)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 482, in hf_raise_for_status\n",
            "    raise _format(HfHubHTTPError, str(e), response) from e\n",
            "huggingface_hub.errors.HfHubHTTPError: 502 Server Error: Bad Gateway for url: https://router.huggingface.co/novita/v3/openai/chat/completions\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Error] 502 Server Error: Bad Gateway for url: https://router.huggingface.co/novita/v3/openai/chat/completions\n",
            "Something went wrong.\n",
            "\n",
            "🧠 Question: Who are you?\n",
            "📄 Context (optional): \n",
            "\n",
            "🔍 Answer:\n",
            "\n",
            "I'm an artificial intelligence model known as a large language model (LLM) or a\n",
            "conversational AI. I'm a computer program designed to simulate human-like\n",
            "conversations, answer questions, provide information, and engage in discussions\n",
            "on a wide range of topics. I'm often referred to as a \"chatbot\" or a \"virtual\n",
            "assistant.\" My primary function is to assist users like you by providing helpful\n",
            "and accurate responses to your queries.  I don't have personal experiences,\n",
            "emotions, or consciousness like humans do. I'm simply a collection of\n",
            "algorithms, data, and software that process and generate text based on the input\n",
            "I receive. My goal is to be informative, neutral, and respectful in my\n",
            "responses, and to help users like you find the information or answers they're\n",
            "looking for.  How can I assist you today?\n",
            "\n",
            "🧠 Question: exit\n",
            "👋 Goodbye, see you next time!\n"
          ]
        }
      ]
    }
  ]
}